# Comparative analysis of Convolutional Neural Network based extraction features in Image captioning

Image captioning is one of the most widely developed applications of deep learning when we discuss its applications in context of convolutional neural networks. Since its advent different computational and associated mathematical researches led to development of different algorithms that has led to the creation of gamut of algorithms and CNN based models each having different salient features and unique logical approach to extract features by converting the image into array form. With rapid rise in use of image captioning in practical applications like self-driving cars, real-time video caption generators and many more the need to have robust feature extraction models with concordantly reduced time complexity and higher accuracy is of prime importance. This will not only enhance model effectivity but at the same time give better user experience. This project deals with an approach to associate the basics of data science with image captioning techniques to analyse the five most widely used feature extractors (VGG16,VGG19,Xception,InceptionV3 and ResNet50) in a comprehensive manner to provide developers an overall picture of each algorithmâ€™s functionality and which algorithm to be used for better model performance.
