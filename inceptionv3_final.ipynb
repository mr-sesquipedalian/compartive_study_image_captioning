{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "inceptionv3_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_RzBEq4irmZ",
        "outputId": "dfc3334e-76d5-42bc-c0bc-a2156bf7777d"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "Note: using Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viBL6kWHinWs"
      },
      "source": [
        "# All paths are relative to train_val.py file\n",
        "config = {\n",
        "\t'images_path': '/content/drive/My Drive/projects/captions/Flicker8k_Dataset/', #Make sure you put that last slash(/)\n",
        "\t'train_data_path': '/content/drive/My Drive/projects/captions/Flickr8k_text/Flickr_8k.trainImages.txt',\n",
        "\t'val_data_path': '/content/drive/My Drive/projects/captions/Flickr8k_text/Flickr_8k.devImages.txt',\n",
        "\t'captions_path': '/content/drive/My Drive/projects/captions/Flickr8k_text/Flickr8k.token.txt',\n",
        "\t'tokenizer_path': '/content/drive/My Drive/projects/captions/data_temp/tokenizer.pkl',\n",
        "\t'model_data_path': '/content/drive/My Drive/projects/captions/data_temp/', #Make sure you put that last slash(/)\n",
        "\t'model_load_path': '/content/drive/My Drive/projects/captions/data_temp/model_inception_v3_epoch-20_train_loss-2.4050_val_loss-3.0527.hdf5',\n",
        "\t'num_of_epochs': 5,\n",
        "\t'max_length': 34, #This is set manually after training of model and required for test.py\n",
        "\t'batch_size': 64,\n",
        "\t'beam_search_k':3,\n",
        "\t'test_data_path': 'test_data/', #Make sure you put that last slash(/)\n",
        "\t'model_type': 'inception_v3', # inception_v3 or vgg16\n",
        "\t'random_seed': 1035\n",
        "}\n",
        "\n",
        "rnnConfig = {\n",
        "\t'embedding_size': 300,\n",
        "\t'LSTM_units': 256,\n",
        "\t'dense_units': 256,\n",
        "\t'dropout': 0.3\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOlSrzR6tBgJ"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from pickle import dump\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# Utility function for pretty printing\n",
        "def mytime(with_date=False):\n",
        "\t_str = ''\n",
        "\tif with_date:\n",
        "\t\t_str = str(dt.now().year)+'-'+str(dt.now().month)+'-'+str(dt.now().day)+' '\n",
        "\t\t_str = _str+str(dt.now().hour)+':'+str(dt.now().minute)+':'+str(dt.now().second)\n",
        "\telse:\n",
        "\t\t_str = str(dt.now().hour)+':'+str(dt.now().minute)+':'+str(dt.now().second)\n",
        "\treturn _str\n",
        "\n",
        "\"\"\"\n",
        "\t*This function returns a dictionary of form:\n",
        "\t{\n",
        "\t\timage_id1 : image_features1,\n",
        "\t\timage_id2 : image_features2,\n",
        "\t\t...\n",
        "\t}\n",
        "\"\"\"\n",
        "def extract_features(path, model_type):\n",
        "\tif model_type == 'inception_v3':\n",
        "\t\tfrom keras.applications.inception_v3 import preprocess_input\n",
        "\t\ttarget_size = (299, 299)\n",
        "\telif model_type == 'vgg16':\n",
        "\t\tfrom keras.applications.vgg16 import preprocess_input\n",
        "\t\ttarget_size = (224, 224)\n",
        "\t# Get CNN Model from model.py\n",
        "\tmodel = CNNModel(model_type)\n",
        "\tfeatures = dict()\n",
        "\t# Extract features from each photo\n",
        "\tfor name in tqdm(os.listdir(path)):\n",
        "\t\t# Loading and resizing image\n",
        "\t\tfilename = path + name\n",
        "\t\timage = load_img(filename, target_size=target_size)\n",
        "\t\t# Convert the image pixels to a numpy array\n",
        "\t\timage = img_to_array(image)\n",
        "\t\t# Reshape data for the model\n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t# Prepare the image for the CNN Model model\n",
        "\t\timage = preprocess_input(image)\n",
        "\t\t# Pass image into model to get encoded features\n",
        "\t\tfeature = model.predict(image, verbose=0)\n",
        "\t\t# Store encoded features for the image\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\tfeatures[image_id] = feature\n",
        "\treturn features\n",
        "\n",
        "\"\"\"\n",
        "\t*Extract captions for images\n",
        "\t*Glimpse of file:\n",
        "\t\t1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
        "\t\t1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
        "\t\t1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n",
        "\t\t1000268201_693b08cb0e.jpg#3\tA little girl climbing the stairs to her playhouse .\n",
        "\t\t1000268201_693b08cb0e.jpg#4\tA little girl in a pink dress going into a wooden cabin .\n",
        "\"\"\"\n",
        "def load_captions(filename):\n",
        "\tfile = open(filename, 'r')\n",
        "\tdoc = file.read()\n",
        "\tfile.close()\n",
        "\t\"\"\"\n",
        "\tCaptions dict is of form:\n",
        "\t{\n",
        "\t\timage_id1 : [caption1, caption2, etc],\n",
        "\t\timage_id2 : [caption1, caption2, etc],\n",
        "\t\t...\n",
        "\t}\n",
        "\t\"\"\"\n",
        "\tcaptions = dict()\n",
        "\t# Process lines by line\n",
        "\t_count = 0\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# Split line on white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# Take the first token as the image id, the rest as the caption\n",
        "\t\timage_id, image_caption = tokens[0], tokens[1:]\n",
        "\t\t# Extract filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# Convert caption tokens back to caption string\n",
        "\t\timage_caption = ' '.join(image_caption)\n",
        "\t\t# Create the list if needed\n",
        "\t\tif image_id not in captions:\n",
        "\t\t\tcaptions[image_id] = list()\n",
        "\t\t# Store caption\n",
        "\t\tcaptions[image_id].append(image_caption)\n",
        "\t\t_count = _count+1\n",
        "\tprint('{}: Parsed captions: {}'.format(mytime(),_count))\n",
        "\treturn captions\n",
        "\n",
        "def clean_captions(captions):\n",
        "\t# Prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor _, caption_list in captions.items():\n",
        "\t\tfor i in range(len(caption_list)):\n",
        "\t\t\tcaption = caption_list[i]\n",
        "\t\t\t# Tokenize i.e. split on white spaces\n",
        "\t\t\tcaption = caption.split()\n",
        "\t\t\t# Convert to lowercase\n",
        "\t\t\tcaption = [word.lower() for word in caption]\n",
        "\t\t\t# Remove punctuation from each token\n",
        "\t\t\tcaption = [w.translate(table) for w in caption]\n",
        "\t\t\t# Remove hanging 's' and 'a'\n",
        "\t\t\tcaption = [word for word in caption if len(word)>1]\n",
        "\t\t\t# Remove tokens with numbers in them\n",
        "\t\t\tcaption = [word for word in caption if word.isalpha()]\n",
        "\t\t\t# Store as string\n",
        "\t\t\tcaption_list[i] =  ' '.join(caption)\n",
        "\n",
        "\"\"\"\n",
        "\t*Save captions to file, one per line\n",
        "\t*After saving, captions.txt is of form :- `id` `caption`\n",
        "\t\tExample : 2252123185_487f21e336 stadium full of people watch game\n",
        "\"\"\"\n",
        "def save_captions(captions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, captions_list in captions.items():\n",
        "\t\tfor caption in captions_list:\n",
        "\t\t\tlines.append(key + ' ' + caption)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "def preprocessData(config):\n",
        "\tprint('{}: Using {} model'.format(mytime(),config['model_type'].title()))\n",
        "\t# Extract features from all images\n",
        "\tif os.path.exists(config['model_data_path']+'features_'+str(config['model_type'])+'.pkl'):\n",
        "\t\tprint('{}: Image features already generated at {}'.format(mytime(), config['model_data_path']+'features_'+str(config['model_type'])+'.pkl'))\n",
        "\telse:\n",
        "\t\tprint('{}: Generating image features using '+str(config['model_type'])+' model...'.format(mytime()))\n",
        "\t\tfeatures = extract_features(config['images_path'], config['model_type'])\n",
        "\t\t# Save to file\n",
        "\t\tdump(features, open(config['model_data_path']+'features_'+str(config['model_type'])+'.pkl', 'wb'))\n",
        "\t\tprint('{}: Completed & Saved features for {} images successfully'.format(mytime(),len(features)))\n",
        "\t# Load file containing captions and parse them\n",
        "\tif os.path.exists(config['model_data_path']+'captions.txt'):\n",
        "\t\tprint('{}: Parsed caption file already generated at {}'.format(mytime(), config['model_data_path']+'captions.txt'))\n",
        "\telse:\n",
        "\t\tprint('{}: Parsing captions file...'.format(mytime()))\n",
        "\t\tcaptions = load_captions(config['captions_path'])\n",
        "\t\t# Clean captions\n",
        "\t\t# Ignore this function because Tokenizer from keras will handle cleaning\n",
        "\t\t# clean_captions(captions)\n",
        "\t\t# Save captions\n",
        "\t\tsave_captions(captions, config['model_data_path']+'captions.txt')\n",
        "\t\tprint('{}: Parsed & Saved successfully'.format(mytime()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1gFTQQNsTU9"
      },
      "source": [
        "import numpy as np\n",
        "from pickle import load, dump\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import random\n",
        "'''\n",
        "\t*We have Flickr_8k.trainImages.txt and Flickr_8k.devImages.txt files which consist of unique identifiers(id) \n",
        "\t\twhich can be used to filter the images and their descriptions\n",
        "\t*Load a pre-defined list of image identifiers(id)\n",
        "\t*Glimpse of file:\n",
        "\t\t2513260012_03d33305cf.jpg\n",
        "\t\t2903617548_d3e38d7f88.jpg\n",
        "\t\t3338291921_fe7ae0c8f8.jpg\n",
        "\t\t488416045_1c6d903fe0.jpg\n",
        "\t\t2644326817_8f45080b87.jpg\n",
        "'''\n",
        "def load_set(filename):\n",
        "\tfile = open(filename, 'r')\n",
        "\tdoc = file.read()\n",
        "\tfile.close()\n",
        "\tids = list()\n",
        "\t# Process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# Skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# Get the image identifier(id)\n",
        "\t\t_id = line.split('.')[0]\n",
        "\t\tids.append(_id)\n",
        "\treturn set(ids)\n",
        "\n",
        "'''\n",
        "\t*The model we'll develop will generate a caption for a given image and the caption will be generated one word at a time. \n",
        "\t*The sequence of previously generated words will be provided as input. Therefore, we will need a ‘first word’ to \n",
        "\t\tkick-off the generation process and a ‘last word‘ to signal the end of the caption.\n",
        "\t*We'll use the strings ‘startseq‘ and ‘endseq‘ for this purpose. These tokens are added to the captions\n",
        "\t\tas they are loaded. \n",
        "\t*It is important to do this now before we encode the text so that the tokens are also encoded correctly.\n",
        "\t*Load captions into memory\n",
        "\t*Glimpse of file:\n",
        "\t\t1000268201_693b08cb0e child in pink dress is climbing up set of stairs in an entry way\n",
        "\t\t1000268201_693b08cb0e girl going into wooden building\n",
        "\t\t1000268201_693b08cb0e little girl climbing into wooden playhouse\n",
        "\t\t1000268201_693b08cb0e little girl climbing the stairs to her playhouse\n",
        "\t\t1000268201_693b08cb0e little girl in pink dress going into wooden cabin\n",
        "'''\n",
        "def load_cleaned_captions(filename, ids):\n",
        "\tfile = open(filename, 'r')\n",
        "\tdoc = file.read()\n",
        "\tfile.close()\n",
        "\tcaptions = dict()\n",
        "\t_count = 0\n",
        "\t# Process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# Split line on white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# Split id from caption\n",
        "\t\timage_id, image_caption = tokens[0], tokens[1:]\n",
        "\t\t# Skip images not in the ids set\n",
        "\t\tif image_id in ids:\n",
        "\t\t\t# Create list\n",
        "\t\t\tif image_id not in captions:\n",
        "\t\t\t\tcaptions[image_id] = list()\n",
        "\t\t\t# Wrap caption in start & end tokens\n",
        "\t\t\tcaption = 'startseq ' + ' '.join(image_caption) + ' endseq'\n",
        "\t\t\t# Store\n",
        "\t\t\tcaptions[image_id].append(caption)\n",
        "\t\t\t_count = _count+1\n",
        "\treturn captions, _count\n",
        "\n",
        "# Load image features\n",
        "def load_image_features(filename, ids):\n",
        "\t# load all features\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {_id: all_features[_id] for _id in ids}\n",
        "\treturn features\n",
        "\n",
        "# Convert a dictionary to a list\n",
        "def to_lines(captions):\n",
        "\tall_captions = list()\n",
        "\tfor image_id in captions.keys():\n",
        "\t\t[all_captions.append(caption) for caption in captions[image_id]]\n",
        "\treturn all_captions\n",
        "\n",
        "'''\n",
        "\t*The captions will need to be encoded to numbers before it can be presented to the model.\n",
        "\t*The first step in encoding the captions is to create a consistent mapping from words to unique integer values.\n",
        "\t\tKeras provides the Tokenizer class that can learn this mapping from the loaded captions.\n",
        "\t*Fit a tokenizer on given captions\n",
        "'''\n",
        "def create_tokenizer(captions):\n",
        "\tlines = to_lines(captions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# Calculate the length of the captions with the most words\n",
        "def calc_max_length(captions):\n",
        "\tlines = to_lines(captions)\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "'''\n",
        "\t*Each caption will be split into words. The model will be provided one word & the image and it generates the next word. \n",
        "\t*Then the first two words of the caption will be provided to the model as input with the image to generate the next word. \n",
        "\t*This is how the model will be trained.\n",
        "\t*For example, the input sequence “little girl running in field” would be \n",
        "\t\tsplit into 6 input-output pairs to train the model:\n",
        "\n",
        "\t\tX1\t\tX2(text sequence) \t\t\t\t\t\t\t\ty(word)\n",
        "\t\t-----------------------------------------------------------------\n",
        "\t\timage\tstartseq,\t\t\t\t\t\t\t\t\t\tlittle\n",
        "\t\timage\tstartseq, little,\t\t\t\t\t\t\t\tgirl\n",
        "\t\timage\tstartseq, little, girl,\t\t\t\t\t\t\trunning\n",
        "\t\timage\tstartseq, little, girl, running,\t\t\t\tin\n",
        "\t\timage\tstartseq, little, girl, running, in,\t\t\tfield\n",
        "\t\timage\tstartseq, little, girl, running, in, field,\t\tendseq\n",
        "'''\n",
        "# Create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, captions_list, image):\n",
        "\t# X1 : input for image features\n",
        "\t# X2 : input for text features\n",
        "\t# y  : output word\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\tvocab_size = len(tokenizer.word_index) + 1\n",
        "\t# Walk through each caption for the image\n",
        "\tfor caption in captions_list:\n",
        "\t\t# Encode the sequence\n",
        "\t\tseq = tokenizer.texts_to_sequences([caption])[0]\n",
        "\t\t# Split one sequence into multiple X,y pairs\n",
        "\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t# Split into input and output pair\n",
        "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t# Pad input sequence\n",
        "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t# Encode output sequence\n",
        "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t# Store\n",
        "\t\t\tX1.append(image)\n",
        "\t\t\tX2.append(in_seq)\n",
        "\t\t\ty.append(out_seq)\n",
        "\treturn X1, X2, y\n",
        "\n",
        "# Data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(images, captions, tokenizer, max_length, batch_size, random_seed):\n",
        "\t# Setting random seed for reproducibility of results\n",
        "\trandom.seed(random_seed)\n",
        "\t# Image ids\n",
        "\timage_ids = list(captions.keys())\n",
        "\t_count=0\n",
        "\tassert batch_size<= len(image_ids), 'Batch size must be less than or equal to {}'.format(len(image_ids))\n",
        "\twhile True:\n",
        "\t\tif _count >= len(image_ids):\n",
        "\t\t\t# Generator exceeded or reached the end so restart it\n",
        "\t\t\t_count = 0\n",
        "\t\t# Batch list to store data\n",
        "\t\tinput_img_batch, input_sequence_batch, output_word_batch = list(), list(), list()\n",
        "\t\tfor i in range(_count, min(len(image_ids), _count+batch_size)):\n",
        "\t\t\t# Retrieve the image id\n",
        "\t\t\timage_id = image_ids[i]\n",
        "\t\t\t# Retrieve the image features\n",
        "\t\t\timage = images[image_id][0]\n",
        "\t\t\t# Retrieve the captions list\n",
        "\t\t\tcaptions_list = captions[image_id]\n",
        "\t\t\t# Shuffle captions list\n",
        "\t\t\trandom.shuffle(captions_list)\n",
        "\t\t\tinput_img, input_sequence, output_word = create_sequences(tokenizer, max_length, captions_list, image)\n",
        "\t\t\t# Add to batch\n",
        "\t\t\tfor j in range(len(input_img)):\n",
        "\t\t\t\tinput_img_batch.append(input_img[j])\n",
        "\t\t\t\tinput_sequence_batch.append(input_sequence[j])\n",
        "\t\t\t\toutput_word_batch.append(output_word[j])\n",
        "\t\t_count = _count + batch_size\n",
        "\t\tyield [[np.array(input_img_batch), np.array(input_sequence_batch)], np.array(output_word_batch)]\n",
        "\n",
        "def loadTrainData(config):\n",
        "\ttrain_image_ids = load_set(config['train_data_path'])\n",
        "\t# Check if we already have preprocessed data saved and if not, preprocess the data.\n",
        "\t# Create and save 'captions.txt' & features.pkl\n",
        "\tpreprocessData(config)\n",
        "\t# Load captions\n",
        "\ttrain_captions, _count = load_cleaned_captions(config['model_data_path']+'captions.txt', train_image_ids)\n",
        "\t# Load image features\n",
        "\ttrain_image_features = load_image_features(config['model_data_path']+'features_'+str(config['model_type'])+'.pkl', train_image_ids)\n",
        "\tprint('{}: Available images for training: {}'.format(mytime(),len(train_image_features)))\n",
        "\tprint('{}: Available captions for training: {}'.format(mytime(),_count))\n",
        "\tif not os.path.exists(config['model_data_path']+'tokenizer.pkl'):\n",
        "\t\t# Prepare tokenizer\n",
        "\t\ttokenizer = create_tokenizer(train_captions)\n",
        "\t\t# Save the tokenizer\n",
        "\t\tdump(tokenizer, open(config['model_data_path']+'tokenizer.pkl', 'wb'))\n",
        "\t# Determine the maximum sequence length\n",
        "\tmax_length = calc_max_length(train_captions)\n",
        "\treturn train_image_features, train_captions, max_length\n",
        "\n",
        "def loadValData(config):\n",
        "\tval_image_ids = load_set(config['val_data_path'])\n",
        "\t# Load captions\n",
        "\tval_captions, _count = load_cleaned_captions(config['model_data_path']+'captions.txt', val_image_ids)\n",
        "\t# Load image features\n",
        "\tval_features = load_image_features(config['model_data_path']+'features_'+str(config['model_type'])+'.pkl', val_image_ids)\n",
        "\tprint('{}: Available images for validation: {}'.format(mytime(),len(val_features)))\n",
        "\tprint('{}: Available captions for validation: {}'.format(mytime(),_count))\n",
        "\treturn val_features, val_captions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p57eM3zgsSxL"
      },
      "source": [
        "import numpy as np\n",
        "# Keras\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, LSTM, Embedding, concatenate, RepeatVector, TimeDistributed, Bidirectional\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "# To measure BLEU Score\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\"\"\"\n",
        "\t*Define the CNN model\n",
        "\"\"\"\n",
        "def CNNModel(model_type):\n",
        "\tif model_type == 'inception_v3':\n",
        "\t\tmodel = InceptionV3()\n",
        "\telif model_type == 'vgg16':\n",
        "\t\tmodel = VGG16()\n",
        "\tmodel.layers.pop()\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\treturn model\n",
        "\n",
        "\"\"\"\n",
        "\t*Define the RNN model\n",
        "\"\"\"\n",
        "def RNNModel(vocab_size, max_len, rnnConfig, model_type):\n",
        "\tembedding_size = rnnConfig['embedding_size']\n",
        "\tif model_type == 'inception_v3':\n",
        "\t\t# inception_v3 outputs a 1000 dimensional vector for each image, which we'll feed to RNN Model\n",
        "\t\timage_input = Input(shape=(2048,))\n",
        "\telif model_type == 'vgg16':\n",
        "\t\t# vgg16 outputs a 4096 dimensional vector for each image, which we'll feed to RNN Model\n",
        "\t\timage_input = Input(shape=(4096,))\n",
        "\timage_model_1 = Dropout(rnnConfig['dropout'])(image_input)\n",
        "\timage_model = Dense(embedding_size, activation='relu')(image_model_1)\n",
        "\n",
        "\tcaption_input = Input(shape=(max_len,))\n",
        "\t# mask_zero: We zero pad inputs to the same length, the zero mask ignores those inputs. E.g. it is an efficiency.\n",
        "\tcaption_model_1 = Embedding(vocab_size, embedding_size, mask_zero=True)(caption_input)\n",
        "\tcaption_model_2 = Dropout(rnnConfig['dropout'])(caption_model_1)\n",
        "\tcaption_model = LSTM(rnnConfig['LSTM_units'])(caption_model_2)\n",
        "\n",
        "\t# Merging the models and creating a softmax classifier\n",
        "\tfinal_model_1 = concatenate([image_model, caption_model])\n",
        "\tfinal_model_2 = Dense(rnnConfig['dense_units'], activation='relu')(final_model_1)\n",
        "\tfinal_model = Dense(vocab_size, activation='softmax')(final_model_2)\n",
        "\n",
        "\tmodel = Model(inputs=[image_input, caption_input], outputs=final_model)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\treturn model\n",
        "\n",
        "\"\"\"\n",
        "\t*Define the RNN model with different architecture\n",
        "\"\"\"\n",
        "def AlternativeRNNModel(vocab_size, max_len, rnnConfig, model_type):\n",
        "\tembedding_size = rnnConfig['embedding_size']\n",
        "\tif model_type == 'inception_v3':\n",
        "\t\t# inception_v3 outputs a 4096 dimensional vector for each image, which we'll feed to RNN Model\n",
        "\t\timage_input = Input(shape=(4096,))\n",
        "\telif model_type == 'inception_v3':\n",
        "\t\t# inception_v3 outputs a 4096 dimensional vector for each image, which we'll feed to RNN Model\n",
        "\t\timage_input = Input(shape=(4096,))\n",
        "\timage_model_1 = Dense(embedding_size, activation='relu')(image_input)\n",
        "\timage_model = RepeatVector(max_len)(image_model_1)\n",
        "\n",
        "\tcaption_input = Input(shape=(max_len,))\n",
        "\t# mask_zero: We zero pad inputs to the same length, the zero mask ignores those inputs. E.g. it is an efficiency.\n",
        "\tcaption_model_1 = Embedding(vocab_size, embedding_size, mask_zero=True)(caption_input)\n",
        "\t# Since we are going to predict the next word using the previous words\n",
        "\t# (length of previous words changes with every iteration over the caption), we have to set return_sequences = True.\n",
        "\tcaption_model_2 = LSTM(rnnConfig['LSTM_units'], return_sequences=True)(caption_model_1)\n",
        "\t# caption_model = TimeDistributed(Dense(embedding_size, activation='relu'))(caption_model_2)\n",
        "\tcaption_model = TimeDistributed(Dense(embedding_size))(caption_model_2)\n",
        "\n",
        "\t# Merging the models and creating a softmax classifier\n",
        "\tfinal_model_1 = concatenate([image_model, caption_model])\n",
        "\t# final_model_2 = LSTM(rnnConfig['LSTM_units'], return_sequences=False)(final_model_1)\n",
        "\tfinal_model_2 = Bidirectional(LSTM(rnnConfig['LSTM_units'], return_sequences=False))(final_model_1)\n",
        "\t# final_model_3 = Dense(rnnConfig['dense_units'], activation='relu')(final_model_2)\n",
        "\t# final_model = Dense(vocab_size, activation='softmax')(final_model_3)\n",
        "\tfinal_model = Dense(vocab_size, activation='softmax')(final_model_2)\n",
        "\n",
        "\tmodel = Model(inputs=[image_input, caption_input], outputs=final_model)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\t# model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\treturn model\n",
        "\n",
        "\"\"\"\n",
        "\t*Map an integer to a word\n",
        "\"\"\"\n",
        "def int_to_word(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "\"\"\"\n",
        "\t*Generate a caption for an image, given a pre-trained model and a tokenizer to map integer back to word\n",
        "\t*Uses simple argmax\n",
        "\"\"\"\n",
        "def generate_caption(model, tokenizer, image, max_length):\n",
        "\t# Seed the generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# Iterate over the whole length of the sequence\n",
        "\tfor _ in range(max_length):\n",
        "\t\t# Integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# Pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\t# Predict next word\n",
        "\t\t# The model will output a prediction, which will be a probability distribution over all words in the vocabulary.\n",
        "\t\tyhat = model.predict([image,sequence], verbose=0)\n",
        "\t\t# The output vector representins a probability distribution where maximum probability is the predicted word position\n",
        "\t\t# Take output class with maximum probability and convert to integer\n",
        "\t\tyhat = np.argmax(yhat)\n",
        "\t\t# Map integer back to word\n",
        "\t\tword = int_to_word(yhat, tokenizer)\n",
        "\t\t# Stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# Append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# Stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text\n",
        "\n",
        "\"\"\"\n",
        "\t*Generate a caption for an image, given a pre-trained model and a tokenizer to map integer back to word\n",
        "\t*Uses BEAM Search algorithm\n",
        "\"\"\"\n",
        "def generate_caption_beam_search(model, tokenizer, image, max_length, beam_index=3):\n",
        "\t# in_text --> [[idx,prob]] ;prob=0 initially\n",
        "\tin_text = [[tokenizer.texts_to_sequences(['startseq'])[0], 0.0]]\n",
        "\twhile len(in_text[0][0]) < max_length:\n",
        "\t\ttempList = []\n",
        "\t\tfor seq in in_text:\n",
        "\t\t\tpadded_seq = pad_sequences([seq[0]], maxlen=max_length)\n",
        "\t\t\tpreds = model.predict([image,padded_seq], verbose=0)\n",
        "\t\t\t# Take top (i.e. which have highest probailities) `beam_index` predictions\n",
        "\t\t\ttop_preds = np.argsort(preds[0])[-beam_index:]\n",
        "\t\t\t# Getting the top `beam_index` predictions and \n",
        "\t\t\tfor word in top_preds:\n",
        "\t\t\t\tnext_seq, prob = seq[0][:], seq[1]\n",
        "\t\t\t\tnext_seq.append(word)\n",
        "\t\t\t\t# Update probability\n",
        "\t\t\t\tprob += preds[0][word]\n",
        "\t\t\t\t# Append as input for generating the next word\n",
        "\t\t\t\ttempList.append([next_seq, prob])\n",
        "\t\tin_text = tempList\n",
        "\t\t# Sorting according to the probabilities\n",
        "\t\tin_text = sorted(in_text, reverse=False, key=lambda l: l[1])\n",
        "\t\t# Take the top words\n",
        "\t\tin_text = in_text[-beam_index:]\n",
        "\tin_text = in_text[-1][0]\n",
        "\tfinal_caption_raw = [int_to_word(i,tokenizer) for i in in_text]\n",
        "\tfinal_caption = []\n",
        "\tfor word in final_caption_raw:\n",
        "\t\tif word=='endseq':\n",
        "\t\t\tbreak\n",
        "\t\telse:\n",
        "\t\t\tfinal_caption.append(word)\n",
        "\tfinal_caption.append('endseq')\n",
        "\treturn ' '.join(final_caption)\n",
        "\n",
        "\"\"\"\n",
        "\t*Evaluate the model on BLEU Score using argmax predictions\n",
        "\"\"\"\n",
        "def evaluate_model(model, images, captions, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor image_id, caption_list in tqdm(captions.items()):\n",
        "\t\tyhat = generate_caption(model, tokenizer, images[image_id], max_length)\n",
        "\t\tground_truth = [caption.split() for caption in caption_list]\n",
        "\t\tactual.append(ground_truth)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\tprint('BLEU Scores :')\n",
        "\tprint('A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.')\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "\"\"\"\n",
        "\t*Evaluate the model on BLEU Score using BEAM search predictions\n",
        "\"\"\"\n",
        "def evaluate_model_beam_search(model, images, captions, tokenizer, max_length, beam_index=3):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor image_id, caption_list in tqdm(captions.items()):\n",
        "\t\tyhat = generate_caption_beam_search(model, tokenizer, images[image_id], max_length, beam_index=beam_index)\n",
        "\t\tground_truth = [caption.split() for caption in caption_list]\n",
        "\t\tactual.append(ground_truth)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\tprint('BLEU Scores :')\n",
        "\tprint('A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.')\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k6AqZFrnho9",
        "outputId": "240e4290-5a0c-47a9-97ce-80e8d1554472"
      },
      "source": [
        "from pickle import load\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import random\n",
        "# Setting random seed for reproducibility of results\n",
        "random.seed(config['random_seed'])\n",
        "\n",
        "\"\"\"\n",
        "    *Some simple checking\n",
        "\"\"\"\n",
        "assert type(config['num_of_epochs']) is int, 'Please provide an integer value for `num_of_epochs` parameter in config.py file'\n",
        "assert type(config['max_length']) is int, 'Please provide an integer value for `max_length` parameter in config.py file'\n",
        "assert type(config['batch_size']) is int, 'Please provide an integer value for `batch_size` parameter in config.py file'\n",
        "assert type(config['beam_search_k']) is int, 'Please provide an integer value for `beam_search_k` parameter in config.py file'\n",
        "assert type(config['random_seed']) is int, 'Please provide an integer value for `random_seed` parameter in config.py file'\n",
        "assert type(rnnConfig['embedding_size']) is int, 'Please provide an integer value for `embedding_size` parameter in config.py file'\n",
        "assert type(rnnConfig['LSTM_units']) is int, 'Please provide an integer value for `LSTM_units` parameter in config.py file'\n",
        "assert type(rnnConfig['dense_units']) is int, 'Please provide an integer value for `dense_units` parameter in config.py file'\n",
        "assert type(rnnConfig['dropout']) is float, 'Please provide a float value for `dropout` parameter in config.py file'\n",
        "\n",
        "\"\"\"\n",
        "\t*Load Data\n",
        "\t*X1 : Image features\n",
        "\t*X2 : Text features(Captions)\n",
        "\"\"\"\n",
        "X1train, X2train, max_length = loadTrainData(config)\n",
        "\n",
        "X1val, X2val = loadValData(config)\n",
        "\n",
        "\"\"\"\n",
        "\t*Load the tokenizer\n",
        "\"\"\"\n",
        "tokenizer = load(open(config['tokenizer_path'], 'rb'))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\"\"\"\n",
        "\t*Now that we have the image features from CNN model, we need to feed them to a RNN Model.\n",
        "\t*Define the RNN model\n",
        "\"\"\"\n",
        "# model = RNNModel(vocab_size, max_length, rnnConfig, config['model_type'])\n",
        "model = RNNModel(vocab_size, max_length, rnnConfig, config['model_type'])\n",
        "print('RNN Model (Decoder) Summary : ')\n",
        "print(model.summary())\n",
        "\n",
        "\"\"\"\n",
        "    *Train the model save after each epoch\n",
        "\"\"\"\n",
        "num_of_epochs = config['num_of_epochs']\n",
        "batch_size = config['batch_size']\n",
        "steps_train = len(X2train)//batch_size\n",
        "if len(X2train)%batch_size!=0:\n",
        "    steps_train = steps_train+1\n",
        "steps_val = len(X2val)//batch_size\n",
        "if len(X2val)%batch_size!=0:\n",
        "    steps_val = steps_val+1\n",
        "model_save_path = config['model_data_path']+\"model_\"+str(config['model_type'])+\"_epoch-{epoch:02d}_train_loss-{loss:.4f}_val_loss-{val_loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks = [checkpoint]\n",
        "\n",
        "print('steps_train: {}, steps_val: {}'.format(steps_train,steps_val))\n",
        "print('Batch Size: {}'.format(batch_size))\n",
        "print('Total Number of Epochs = {}'.format(num_of_epochs))\n",
        "\n",
        "# Shuffle train data\n",
        "ids_train = list(X2train.keys())\n",
        "random.shuffle(ids_train)\n",
        "X2train_shuffled = {_id: X2train[_id] for _id in ids_train}\n",
        "X2train = X2train_shuffled\n",
        "\n",
        "# Create the train data generator\n",
        "# returns [[img_features, text_features], out_word]\n",
        "generator_train = data_generator(X1train, X2train, tokenizer, max_length, batch_size, config['random_seed'])\n",
        "# Create the validation data generator\n",
        "# returns [[img_features, text_features], out_word]\n",
        "generator_val = data_generator(X1val, X2val, tokenizer, max_length, batch_size, config['random_seed'])\n",
        "\n",
        "# Fit for one epoch\n",
        "model.fit_generator(generator_train,epochs=num_of_epochs,steps_per_epoch=steps_train,validation_data=generator_val,validation_steps=steps_val,\n",
        "                    callbacks=callbacks,verbose=1)\n",
        "\n",
        "\"\"\"\n",
        "\t*Evaluate the model on validation data and output BLEU score\n",
        "\"\"\"\n",
        "print('Model trained successfully. Running model on validation set for calculating BLEU score using BEAM search with k={}'.format(config['beam_search_k']))\n",
        "evaluate_model_beam_search(model, X1val, X2val, tokenizer, max_length, beam_index=config['beam_search_k'])\n",
        "evaluate_model(model, X1val, X2val, tokenizer, max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6:7:5: Using Inception_V3 model\n",
            "6:7:5: Image features already generated at /content/drive/My Drive/projects/captions/data_temp/features_inception_v3.pkl\n",
            "6:7:5: Parsed caption file already generated at /content/drive/My Drive/projects/captions/data_temp/captions.txt\n",
            "6:7:5: Available images for training: 6000\n",
            "6:7:5: Available captions for training: 30000\n",
            "6:7:5: Available images for validation: 1000\n",
            "6:7:5: Available captions for validation: 5000\n",
            "RNN Model (Decoder) Summary : \n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 40)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, 2048)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 40, 300)      2213400     input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 2048)         0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 40, 300)      0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 300)          614700      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 256)          570368      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 556)          0           dense_7[0][0]                    \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 256)          142592      concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 7378)         1896146     dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,437,206\n",
            "Trainable params: 5,437,206\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "steps_train: 94, steps_val: 16\n",
            "Batch Size: 64\n",
            "Total Number of Epochs = 5\n",
            "Epoch 1/5\n",
            "94/94 [==============================] - 2012s 21s/step - loss: 5.4382 - val_loss: 4.5236\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.52359, saving model to /content/drive/My Drive/projects/captions/data_temp/model_inception_v3_epoch-01_train_loss-5.4413_val_loss-4.5236.hdf5\n",
            "Epoch 2/5\n",
            "94/94 [==============================] - 1890s 20s/step - loss: 4.1579 - val_loss: 3.8057\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.52359 to 3.80570, saving model to /content/drive/My Drive/projects/captions/data_temp/model_inception_v3_epoch-02_train_loss-4.1594_val_loss-3.8057.hdf5\n",
            "Epoch 3/5\n",
            "94/94 [==============================] - 1827s 19s/step - loss: 3.6151 - val_loss: 3.5258\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.80570 to 3.52584, saving model to /content/drive/My Drive/projects/captions/data_temp/model_inception_v3_epoch-03_train_loss-3.6159_val_loss-3.5258.hdf5\n",
            "Epoch 4/5\n",
            "94/94 [==============================] - 1811s 19s/step - loss: 3.3207 - val_loss: 3.3758\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.52584 to 3.37578, saving model to /content/drive/My Drive/projects/captions/data_temp/model_inception_v3_epoch-04_train_loss-3.3214_val_loss-3.3758.hdf5\n",
            "Epoch 5/5\n",
            "94/94 [==============================] - 1777s 19s/step - loss: 3.1204 - val_loss: 3.2881\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.37578 to 3.28808, saving model to /content/drive/My Drive/projects/captions/data_temp/model_inception_v3_epoch-05_train_loss-3.1211_val_loss-3.2881.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model trained successfully. Running model on validation set for calculating BLEU score using BEAM search with k=3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [15:34<00:00,  1.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU Scores :\n",
            "A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.\n",
            "BLEU-1: 0.581191\n",
            "BLEU-2: 0.329493\n",
            "BLEU-3: 0.226465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 2/1000 [00:00<01:32, 10.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-4: 0.112489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:33<00:00, 10.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU Scores :\n",
            "A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.\n",
            "BLEU-1: 0.569917\n",
            "BLEU-2: 0.317177\n",
            "BLEU-3: 0.208284\n",
            "BLEU-4: 0.096259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ioAQhUZkXmW",
        "outputId": "b7489208-5919-469b-9c4a-5c2fefaf3351"
      },
      "source": [
        "pip install keras==2.2.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.18.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN-CsxbHkbv9",
        "outputId": "b797e427-b568-4b2d-c65c-620412cb9817"
      },
      "source": [
        "pip install tensorflow==1.13.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 49kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 52.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.33.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.35.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.3)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (50.3.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHWbaDkbkyQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d276d4e-72a2-4cd3-a129-cb7ab7d8d690"
      },
      "source": [
        "pip uninstall tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e-0PWSz8otr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}